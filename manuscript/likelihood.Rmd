---
title: "likelihood"
author: "Mark Hagemann"
date: "November 16, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


What information do the observed $\delta A_{it}$'s provide about $A_{0, i}$? Some, surely. We know, for instance, that $A_{0, i} > 0 \implies A_{it} > \delta A_{it}$. Beyond that, we must turn to probability, and assume some probability distribution associated with $A_{it}$. This may be seen as a crutch, since erroneously making such an assumption can produce spurious results. However, if properly stated, this can lead to the correct answer. For now, we shall leave aside the question of which distribution to use, and proceed generically as far as possible.

## Likelihood and Bayes

This section will expand beyond the linear model representation of McFLI into a more general definition of parameter identifiability based on probability and likelihood. 

Likelihood provides a probabilistically motivated means of inferring parameters from data. Apropos to the interests of this paper it also characterizes the degree of uncertainty corresponding to a particular paramater set. 

The observed data, $\mathbf{y}$ are observations of a stochastic process that is governed by a set of parameters, $\mathbf{\theta}$. The probability of any particular set of observations $\mathbf{y^*}$ is given by a distribution function $f(\mathbf{y^*} | \theta)$. Values of $\mathbf{y}$ that are more probable yield larger values of $f(\mathbf{y} | \theta)$. Since parameters are not observed but must be inferred from the data, it is desirable to distinguish good estimates from bad ones.  Parameter estimates ($\hat{\theta}$) that result in the observed data ($\mathbf{y_{obs}}$) being more probable are more *likely*. Thus, *likelihood* is formally defined as any function of $\hat{\theta}$ that is directly proportional to $f(\mathbf{y_{obs}} | \hat{\theta})$:


$$
L(\hat{\theta} |\mathbf{y_{obs}}) \propto f(\mathbf{y_{obs}} | \hat{\theta})
$$
Whereas $f()$ is a function of random outcomes conditional on known parameters, $L$ is a function of unknown parameters conditional on observed outcomes. 

Given a likelihood and a set of observations, $\theta$ can be inferred by several methods.  $L$ can be maximized (analytically or numerically) over the set of all possible $\theta$ to obtain the maximum likelihood estimate of $\theta$. Alternatively, Bayesian inference may be performed to obtain a full *posterior* distribution of $\theta$ representing its uncertainty *given* the both the data and its *prior* uncertianty, $\pi(\theta)$:
$$
f(\theta | \mathbf{y_{obs}}) = \frac{L(\theta | \mathbf{y_{oobs}})\pi(\theta)}{\int_\Theta L(\theta' | \mathbf{y_{oobs}})\pi(\theta')d\theta'}
$$
The remainder of this section will employ likelihood theory to McFLI models in order to more fully ascertain the information content of SWOT-like observations regarding Mannings' equation parameters and discharge.  Section #### will demonstrate the same conclusions as section #### using likelihood. Section #### will do the same for the model from section ####, but reach a more nuanced and complete conclusion that is far less bleak. 









## "Optimistic" model likelihood

The "optimistic" case described in section #### may be considered a stochastic process if it is recognized that the observations are not obtained from the physics underlying Manning's equation, but from a sum of physics and multiple sources of error. This error manifests as a random variable, $\epsilon_{it}$, assumed here to be centered at zero and to have constant covariance $\Sigma$. 
$$
y_{it} = \tilde{n} + \bar{q} + \dot{q_t} + \epsilon_{it}
$$
Since we are already being overly optimistic, we will further afford ourselves the simplifying assumption that the errors are independent in time and space, i.e. $\Sigma = \sigma^2 I$, where $\sigma^2$ is a constant and $I$ is the identity matrix. This ensures that the set of observed data contain the maximum information. In this case we can write the log-likelihood for the parameters given the data as: 


$$
\ell(\tilde{n}, \bar{q}, \dot{q_t}) = \sum_{i,t}\log f(y_{it} | \tilde{n}, \bar{q}, \dot{q_t})
$$
To show that this model is not identifiable, it is sufficient to show that $f(y | y_{it} = \tilde{n}, \bar{q}, \dot{q_t}) = f(y | \tilde{n}', \bar{q}', \dot{q_t}), \tilde{n} \ne \tilde{n}', \bar{q} \ne \bar{q}'$. For example 





## "Pessimistic" model likelihood



As noted, equation #### can be repraramterized as follows:
$$
y_{it} = \tilde{n} + \bar{q} + \dot{q_t} + a_{it} + \epsilon_{it}
$$

where $\epsilon_{it}$ is a random variable centered at zero and with constant covariance $\Sigma$. *Given* that we know $\tilde{n}, \bar{q}, \dot{q_t}, a_{it} \text{ and } \Sigma$, then $y_{it}$ is a random variable centered at $\mu_{it} = \tilde{n} + \bar{q} + \dot{q_t} + a_{it}$ and with constant covariance $\Sigma$. Therefore the likelihood, $L$ of the parameters $\{ \tilde{n}, \bar{q}, \dot{q_t}, a_{it}\}$ is equal to the probability density, $f$, of $y_{it}$ given these values. 
$$
L(\tilde{n}, \bar{q}, \dot{q_t}, a_{it}, \Sigma) = f(y_{it} | \tilde{n}, \bar{q}, \dot{q_t}, a_{it}, \Sigma)
$$

If $y_{it}$ are all independent from one another, then the joint pdf is equalt to the product of the 



Going forward, we will further assume $\epsilon_{it}$ to be normally distributed, although the conclusions of this section are applicable for any symmetric distribution fully parameterized by mean and covariance. 

The log-likeilihood is then 

$$
\begin{aligned}
\ell(\tilde{n}, \bar{q}, \dot{q_t}, a_{it}, \Sigma) &= -\frac{1}{2} \log |\Sigma| -\frac{1}{2} (y_{it} - \mu_{it})'\Sigma^{-1}(y_{it} - \mu_{it}) \\
&= -\frac{1}{2} \log |\Sigma| -\frac{1}{2} (y_{it} - (\tilde{n} + \bar{q} + \dot{q_t} + a_{it}))'\Sigma^{-1}(y_{it} - (\tilde{n} + \bar{q} + \dot{q_t} + a_{it}))
\end{aligned}
$$

We can find maximum likelihood estimates of several of these parameters by marginalizing over this function:

$$
\begin{aligned}
\frac{\partial \ell}{\partial \dot{q_t}} &= - \Sigma^{-1}(y_{it} - \mu_{it}) \\
& = - \Sigma^{-1}(y_{it} - \dot{q_t}) + C
\end{aligned}
$$

This is zero when $\dot{q_t} = \bar{y}_{\cdot t}$. Further, the second derivative of $\ell(\dot{q_t})$ is 

## Previous work



Let $f_i(A_{i1}, A_{i2}, \dots, A_{iN_t} | \theta_i)$ be the joint probability distribution governing $\mathbf{A}_{i}$, contitional on location-specific distributional parameters $\theta_i$. Then the likelihood of any particular $\theta_i$ is the same, only considered as a function of $\theta_i$ and conditional on the data, $A_{i1}, A_{i2}, \dots, A_{iN_t}$. 

$$
L(\theta_i | A_{i1}, A_{i2}, \dots, A_{iN_t}) = f_i(A_{i1}, A_{i2}, \dots, A_{iN_t} | \theta_i)
$$

For any set of observations, then, we could obtain a maximum likelihood estimate for $\theta_i$, assuming such a maximum exists. 


When choosing a distribution for $A_{it}$, we'll consider the following:

- $A_{it}$ is strictly positive.
- $A_{it}$ is likely to be right-skewed, as rivers experience infrequent but very large cross-sectional areas during storm / flood stages. 
- The log-transform of $A_{it}$ appears among additive terms in equation @eq:Mannings3. 

These features--positivity, right-skew, multiplicative governing process--lead naturally to the lognormal distribution for a likelihood function. Its parameters are $\theta_i = \{\mu_i, \sigma^2_i\}$--the mean and variance, respectively, of $\log{\mathbf{A}_i}$. An additional parameter, $\rho_i$, is further required to specify the temporal autocorrelation of $\mathbf{A}_i$ Under this assumption, we obtain the following likelihood 


The pdf of $A$ is then:

$$
f_A(x) = \frac{1}{x \sqrt{2 \pi} \sigma_A} \exp{\big(-\frac{1}{2 \sigma_A^2}[\log x - \mu_A]^2\big)}
$$

Changing variables to $\delta A = A - A_0$, we obtain the following distribution for $\delta A$:


$$
f_{\delta A}(x) = \frac{1}{(x + A_0) \sqrt{2 \pi} \sigma_A} \exp{\big(-\frac{1}{2 \sigma_A^2}[\log (x + A_0) - \mu_A]^2\big)}
$$

This gives a likelihood function with 3 quantities of interest: $A_0$, $\mu_A$, and $\sigma_A$. However, recall that we can choose $A_0$ to be *any time-invariant quantity*. Further, $\mu_A$ is the median of $\log A$, implying that $e^{\mu_a}$ is the median of $A$. Thus if we choose $\mu_A = A_0$, we can effectively reduce the number of parameters in the above likelihood by 1, yielding: 

$$
f_{\delta A | \sigma_A, A_0}(x |  \sigma_A, A_0) = \frac{1}{(x + A_0) \sqrt{2 \pi} \sigma_A} \exp{\big(-\frac{1}{2 \sigma_A^2}[\log (x + A_0) - \log A_0]^2\big)}, (x > -A_0)
$$

Inference for $\sigma_A$ and $A_0$ is conducted via the log-likelihood:

$$
\ell(\sigma_A, A_0) = \sum_{i = 1}^{N_t} \Big(- \log {(x_i + A_0)} -  \frac{1}{2} \log{\sigma_A^2} - \frac{1}{2 \sigma_A^2}[\log (x_i + A_0) - \log A_0]^2 \Big)
$$



For instance, we can find the maximum likelihood estimates of these parameters via differentiation:


$$
\begin{aligned}
\frac{\partial \ell}{\partial A_0} &= \sum_{i = 1}^{N_t} \Big( -\frac{1}{x_i + A_0}  - \frac{1}{ \sigma_A^2}[\log (x_i + A_0) - \log A_0](\frac{1}{x_i + A_0} - \frac{1}{A_0}) \Big) \\
&= \sum_{i = 1}^{N_t} \Big(  \frac{x_i \log A_0 - \sigma^2_A A_0 - x_i\log (x_i + A_0) }{\sigma_{A}^2A_0 (x_i + A_0)} \Big) \\
&=  \sum_{i = 1}^{N_t} \Big(  \frac{x_i \log A_0}{\sigma_{A}^2A_0 (x_i + A_0)} - 
\frac{ \sigma^2_A A_0}{\sigma_{A}^2A_0 (x_i + A_0)} - 
\frac{ x_i\log (x_i + A_0) }{\sigma_{A}^2A_0 (x_i + A_0)} \Big)\\
&= \sum_{i = 1}^{N_t}   \frac{x_i \log A_0}{\sigma_{A}^2A_0 (x_i + A_0)} - 
 \sum_{i = 1}^{N_t} \frac{ 1}{ x_i + A_0} - 
 \sum_{i = 1}^{N_t} \frac{ x_i\log (x_i + A_0) }{\sigma_{A}^2A_0 (x_i + A_0)} \\
\end{aligned}
$$

This is proving intractable to maximize analytically. 





Here we shall consider the distribution of random variables. Convention dictates that random variables be denoted using capital letters and their realizations (data) be denoted in lowercase. Since we have already represented data in uppercase, herein we instead use boldface to denote random variables, and non-bold for data. 